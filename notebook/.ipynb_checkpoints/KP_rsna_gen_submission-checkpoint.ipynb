{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Added a preconverted jpg data library to kernel from https://www.kaggle.com/vaillant/discussion see explanation for RGB channels<BR>\n",
    "Starter code from https://www.kaggle.com/orkatz2/pulmonary-embolism-pytorch-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:08.185192Z",
     "start_time": "2020-10-23T14:34:08.173646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import functools\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gc\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import pydicom\n",
    "from snippets import reduce_mem_usage\n",
    "from snippets import config\n",
    "from snippets import transforms_val\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:08.230048Z",
     "start_time": "2020-10-23T14:34:08.190468Z"
    }
   },
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:08.250100Z",
     "start_time": "2020-10-23T14:34:08.232975Z"
    }
   },
   "outputs": [],
   "source": [
    "#where the data is\n",
    "PATH = '../input/rsna-str-pulmonary-embolism-detection'\n",
    "PATH_TEST=PATH+'/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:08.266894Z",
     "start_time": "2020-10-23T14:34:08.252803Z"
    }
   },
   "outputs": [],
   "source": [
    "# old dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T16:10:04.605360Z",
     "start_time": "2020-10-21T16:10:04.600249Z"
    }
   },
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:08.432662Z",
     "start_time": "2020-10-23T14:34:08.269263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n",
      "146853\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"../input/rsna-str-pulmonary-embolism-detection/test.csv\",dtype={'StudyInstanceUID':'string', 'SeriesInstanceUID':'string', 'SOPInstanceUID':'string'})\n",
    "# test_df = test_df.set_index('SOPInstanceUID')\n",
    "listOfStudyID = test_df['StudyInstanceUID'].unique()\n",
    "print(len(listOfStudyID))\n",
    "print(len(test_df))\n",
    "\n",
    "# test_df.head()\n",
    "# test_df.describe()pydicom\n",
    "# test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T14:34:10.920541Z",
     "start_time": "2020-10-23T14:34:10.816863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudyInstanceUID     string\n",
       "SeriesInstanceUID    string\n",
       "SOPInstanceUID       string\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a subset of the data for test\n",
    "If you want to test on a subset run this code to get a smaller dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.df_cols['pe_present_on_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1694054   96540\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "### TRAINING DATA CHECK\n",
    "\n",
    "train_df = pd.read_csv(\"../input/rsna-str-pulmonary-embolism-detection/train.csv\")\n",
    "# train_df = reduce_mem_usage(train_df)\n",
    "# train_df[ train_df['pe_present_on_image']==1]\n",
    "\n",
    "df0 = train_df[train_df['pe_present_on_image']==0]\n",
    "df1 = train_df[train_df['pe_present_on_image']==1]\n",
    "print( str(len(df0))+\"   \" + str(len(df1)))\n",
    "\n",
    "test_df = pd.concat([df0[:5000],df1[:5000]],axis=0)\n",
    "print(len(test_df))\n",
    "\n",
    "test_df[test_df['pe_present_on_image']==1].head()\n",
    "\n",
    "#if using train then indicate that you look in train dir\n",
    "jpeg_dir = '../input/rsna-str-pe-detection-jpeg-256/train-jpegs'\n",
    "PATH_TEST=jpeg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(listOfStudyID))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.389312Z",
     "start_time": "2020-10-23T04:52:05.381904Z"
    }
   },
   "outputs": [],
   "source": [
    "from snippets import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.400764Z",
     "start_time": "2020-10-23T04:52:05.390248Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_df, batch_size=config.batch_size*2, shuffle=False, num_workers=config.WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.419930Z",
     "start_time": "2020-10-23T04:52:05.401723Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# listOfStudyID[0]\n",
    "# thisStudyDF = test_df[test_df['StudyInstanceUID']==listOfStudyID[0]]\n",
    "# len(thisStudyDF)\n",
    "# # thisStudyDF\n",
    "\n",
    "# eachImageID=thisStudyDF.index[0]\n",
    "# import pydicom\n",
    "\n",
    "# # eachImagePath = '../input/rsna-str-pulmonary-embolism-detection/test/'+test_df.loc[eachImageID, 'StudyInstanceUID']+'/'+test_df.loc[eachImageID, 'SeriesInstanceUID']+'/'+eachImageID+'.dcm'\n",
    "# eachImagePath = '../input/rsna-str-pulmonary-embolism-detection/test/00268ff88746/75d23269adbd/012c12fe09c3.dcm'\n",
    "# dcm_data = pydicom.dcmread(eachImagePath)\n",
    "# image = dcm_data.pixel_array * int(dcm_data.RescaleSlope) + int(dcm_data.RescaleIntercept)\n",
    "# image = np.stack([test_dataset._window(image, WL=-600, WW=1500),\n",
    "#                   test_dataset._window(image, WL=40, WW=400),\n",
    "#                   test_dataset._window(image, WL=100, WW=700)], 2)\n",
    "\n",
    "# print(image.shape)\n",
    "# # image = image.astype(np.float32)\n",
    "# image = preprocessing_val(image)\n",
    "# toPred = image.unsqueeze(0)\n",
    "# print(image.shape)\n",
    "# # z = model(toPred)\n",
    "# # pred = torch.sigmoid(z)\n",
    "# # pred1 = pred.detach().numpy().astype('float32')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.439582Z",
     "start_time": "2020-10-23T04:52:05.429157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#where should it go\n",
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.641128Z",
     "start_time": "2020-10-23T04:52:05.440481Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model that was just trained\n",
    "#and do following inference with it\n",
    "model = models.resnet18(pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features,config.numb_classes)\n",
    "\n",
    "#it was saved on GPU, load it on CPU\n",
    "model.load_state_dict(torch.load(config.MODEL_PARAMS_NEW_LOC, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction generator\n",
    "\n",
    "Goes in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T04:52:05.662991Z",
     "start_time": "2020-10-23T04:52:05.651809Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# exam_level_features = ['negative_exam_for_pe', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
    "#                        'leftsided_pe',         'chronic_pe',        'rightsided_pe', \n",
    "#                        'acute_and_chronic_pe', 'central_pe',        'indeterminate']\n",
    "\n",
    "# #create columns\n",
    "# index = ['StudyInstanceUID']\n",
    "\n",
    "# all = (index)\n",
    "# all.extend(exam_level_features)\n",
    "\n",
    "# #create dict of unique StudyInstanceUID\n",
    "# studies={'StudyInstanceUID':listOfStudyID}\n",
    "\n",
    "# totals_per_series = pd.DataFrame(studies,columns=all).fillna(0.0)\n",
    "\n",
    "# %time\n",
    "# from snippets import SInstUID_tracker\n",
    "\n",
    "# import time\n",
    "# start_time = time.perf_counter()\n",
    " \n",
    "# f = open('submission.csv', 'w')\n",
    "# f.write('id,label\\n')\n",
    "\n",
    "# sidt = SInstUID_tracker(test_df )\n",
    "# imgs_processed=0\n",
    "\n",
    "# # dev = 'cpu'\n",
    "# model.to(dev)\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     for eachStudyID in tqdm(listOfStudyID):\n",
    "        \n",
    "#         #get one study\n",
    "#         thisStudyDF = test_df[test_df['StudyInstanceUID']==eachStudyID]\n",
    "        \n",
    "#         #create a dataset from that study\n",
    "#         test_dataset = CTDatasetDicom(thisStudyDF,path= PATH_TEST, transforms=transforms_val)\n",
    "        \n",
    "#         #create a dataloader for just that study\n",
    "#         test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=config.WORKERS, pin_memory=True)        \n",
    "# #         test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size*4, shuffle=False, num_workers=config.WORKERS, pin_memory=True)\n",
    "# #         tqdm_loader = tqdm(test_dataloader)\n",
    "#         for idx, images in enumerate(test_dataloader):\n",
    "#             images=images.to(dev)\n",
    "# #             print(len(images))\n",
    "            \n",
    "#             pred=model(images)\n",
    "#             pred = torch.sigmoid(pred)\n",
    "            \n",
    "#             print(pred)\n",
    "#             raise\n",
    "            \n",
    "#             imgs_processed+=64\n",
    "#             if(imgs_processed>1000):\n",
    "#                 elapsed_time = time.perf_counter() - start_time\n",
    "#                 tpall =((elapsed_time/1000.0)*1500000)/60.0\n",
    "#                 print(f'time to do 1.5 million images {tpall:0.4f} minutes')\n",
    "                #its 574 minutes for CPU\n",
    "                #its 111 minutes with bathcsize of 64 on GPU\n",
    "                #with only 1 data_loader for the whole set its 90 minutes\n",
    "#             print(pred)\n",
    "#             raise\n",
    "            \n",
    "            \n",
    "            \n",
    "#         for eachImageID in thisStudyDF.index:\n",
    "            \n",
    "# #             try:\n",
    "#                 eachImagePath = '../input/rsna-str-pulmonary-embolism-detection/test/'+test_df.loc[eachImageID, 'StudyInstanceUID']+'/'+test_df.loc[eachImageID, 'SeriesInstanceUID']+'/'+eachImageID+'.dcm'\n",
    "#                 dcm_data = dcmread(eachImagePath)\n",
    "#                 image = dcm_data.pixel_array * int(dcm_data.RescaleSlope) + int(dcm_data.RescaleIntercept)\n",
    "#                 image = np.stack([window(image, WL=-600, WW=1500),\n",
    "#                                   window(image, WL=40, WW=400),\n",
    "#                                   window(image, WL=100, WW=700)], 2)\n",
    "\n",
    "#                 image = image.astype(np.float32)\n",
    "#                 image = data_transform(image)\n",
    "#                 toPred = image.unsqueeze(0).cuda()\n",
    "#                 z = model(toPred)\n",
    "#                 pred = torch.sigmoid(z)\n",
    "#                 pred = pred.cpu().detach().numpy().astype('float32')[0,0]\n",
    "\n",
    "#             except:\n",
    "#                 pred = defaultScore['_pe_present_on_image']\n",
    "          \n",
    "\n",
    "# df_image_results=pd.DataFrame( columns=['image','pe_present_on_image' ])\n",
    "# df_image_results=df_image_results.append({'image':'1 image', 'pe_present_on_image':5.0}, ignore_index=True)\n",
    "# df_image_results=df_image_results.append({'image':'2 image', 'pe_present_on_image':5.0}, ignore_index=True)\n",
    "# df_image_results=df_image_results.append({'image':'3 image', 'pe_present_on_image':5.0}, ignore_index=True)\n",
    "\n",
    "# df_image_results.head()\n",
    "\n",
    "# df1 = self.df_main[self.df_main[:,3]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick one dataset type\n",
    "from snippets import CTDatasetJPEG\n",
    "test_dataset = CTDatasetJPEG(test_df,path= PATH_TEST, transforms=transforms_val, mode='val')\n",
    "\n",
    "# from snippets import CTDatasetJPEG\n",
    "# test_dataset = CTDatasetDicom(test_df,path= PATH_TEST, transforms=transforms_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-23T04:52:05.226Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [03:00<00:00,  4.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from snippets import SInstUID_tracker\n",
    "import time\n",
    "start_time = time.perf_counter()\n",
    " \n",
    "# imgs_processed=0\n",
    "df_image_results=pd.DataFrame( columns=['imageSOP','pe_present_on_image' ])\n",
    "\n",
    "# dev = 'cpu'\n",
    "model.to(dev)\n",
    "with torch.no_grad():      \n",
    "    sidt = SInstUID_tracker(test_df )\n",
    "    \n",
    "    #create a dataloader for just that study\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=config.WORKERS, pin_memory=True)        \n",
    "   \n",
    "    start_time = time.perf_counter()\n",
    "    tqdm_loader = tqdm(test_dataloader)\n",
    "    for idx, (studyids,imageSOP,images,_) in enumerate(tqdm_loader):\n",
    "        images=images.to(dev)\n",
    "\n",
    "        preds=model(images)\n",
    "        preds=torch.sigmoid(preds).cpu()\n",
    "       \n",
    "        for i,pred in enumerate(preds):\n",
    "            pred=pred.numpy()\n",
    "            sidt.record(studyids[i],pred)\n",
    "            #the following line greatly increases time to run from 88 to 104 minutes and it leaks\n",
    "            df_image_results=df_image_results.append({'imageSOP':imageSOP[i], 'pe_present_on_image':preds[0]}, ignore_index=True)\n",
    "\n",
    "        del images,studyids,imageSOP\n",
    "        \n",
    "#         imgs_processed+=256\n",
    "#         if(imgs_processed>20000):\n",
    "#             elapsed_time = time.perf_counter() - start_time\n",
    "#             tpall =((elapsed_time/10000.0)*1500000)/60.0\n",
    "#             print(f'time to do 1.5 million images {tpall:0.4f} minutes')\n",
    "\n",
    "# with open('submission.txt' as f):\n",
    "#     f.write(\"info\")\n",
    "#     for each row in sidt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and save a dataframe\n",
    "sidt.df.to_pickle(\"./sidt_df_jpeg.pkl\")\n",
    "sidt_new = SInstUID_tracker(test_df )\n",
    "sidt_new.df= pd.read_pickle(\"./sidt_df_jpeg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidt_new.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "      <th>negative_exam_for_pe</th>\n",
       "      <th>qa_motion</th>\n",
       "      <th>qa_contrast</th>\n",
       "      <th>flow_artifact</th>\n",
       "      <th>rv_lv_ratio_gte_1</th>\n",
       "      <th>rv_lv_ratio_lt_1</th>\n",
       "      <th>leftsided_pe</th>\n",
       "      <th>chronic_pe</th>\n",
       "      <th>true_filling_defect_not_pe</th>\n",
       "      <th>rightsided_pe</th>\n",
       "      <th>acute_and_chronic_pe</th>\n",
       "      <th>central_pe</th>\n",
       "      <th>indeterminate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018426</td>\n",
       "      <td>0.997722</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.041203</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.034728</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.000648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029482</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.008538</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.996599</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.997907</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.997907</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.997907</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.040730</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.034396</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.006748</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.000647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040358</td>\n",
       "      <td>0.998848</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.187010</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.056217</td>\n",
       "      <td>0.131528</td>\n",
       "      <td>0.024690</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.030753</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SeriesInstanceUID  SOPInstanceUID  pe_present_on_image  \\\n",
       "count              175.0           175.0           175.000000   \n",
       "mean                 0.0             0.0             0.018426   \n",
       "std                  0.0             0.0             0.002236   \n",
       "min                  0.0             0.0             0.018091   \n",
       "25%                  0.0             0.0             0.018091   \n",
       "50%                  0.0             0.0             0.018091   \n",
       "75%                  0.0             0.0             0.018091   \n",
       "max                  0.0             0.0             0.040358   \n",
       "\n",
       "       negative_exam_for_pe   qa_motion  qa_contrast  flow_artifact  \\\n",
       "count            175.000000  175.000000   175.000000     175.000000   \n",
       "mean               0.997722    0.000703     0.000524       0.041203   \n",
       "std                0.000479    0.000005     0.000004       0.029482   \n",
       "min                0.996599    0.000703     0.000524       0.027513   \n",
       "25%                0.997907    0.000703     0.000524       0.027513   \n",
       "50%                0.997907    0.000703     0.000524       0.027513   \n",
       "75%                0.997907    0.000703     0.000524       0.040730   \n",
       "max                0.998848    0.000769     0.000574       0.187010   \n",
       "\n",
       "       rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  leftsided_pe  chronic_pe  \\\n",
       "count         175.000000        175.000000    175.000000  175.000000   \n",
       "mean            0.001889          0.034728      0.022595    0.007090   \n",
       "std             0.000071          0.002371      0.008538    0.001917   \n",
       "min             0.001877          0.034396      0.021668    0.006748   \n",
       "25%             0.001877          0.034396      0.021668    0.006748   \n",
       "50%             0.001877          0.034396      0.021668    0.006748   \n",
       "75%             0.001877          0.034396      0.021668    0.006748   \n",
       "max             0.002493          0.056217      0.131528    0.024690   \n",
       "\n",
       "       true_filling_defect_not_pe  rightsided_pe  acute_and_chronic_pe  \\\n",
       "count                  175.000000     175.000000            175.000000   \n",
       "mean                     0.001989       0.011893              0.000876   \n",
       "std                      0.000087       0.001912              0.000107   \n",
       "min                      0.001980       0.011601              0.000858   \n",
       "25%                      0.001980       0.011601              0.000858   \n",
       "50%                      0.001980       0.011601              0.000858   \n",
       "75%                      0.001980       0.011601              0.000858   \n",
       "max                      0.003064       0.030753              0.002035   \n",
       "\n",
       "       central_pe  indeterminate  \n",
       "count  175.000000     175.000000  \n",
       "mean     0.003140       0.000648  \n",
       "std      0.000171       0.000005  \n",
       "min      0.003118       0.000647  \n",
       "25%      0.003118       0.000647  \n",
       "50%      0.003118       0.000647  \n",
       "75%      0.003118       0.000647  \n",
       "max      0.004839       0.000712  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidt.df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidt.df[sidt.df['pe_present_on_image']>0.04].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission.csv', 'w')\n",
    "f.write('id,label\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# for img, lbl in train_dl:\n",
    "#     print(img[0].shape)\n",
    "#     print(len(lbl[0]))\n",
    "#     break\n",
    "# # tmp=next((train_dl))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# row = train_dataset.df[0]\n",
    "# print(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")\n",
    "# img = cv2.imread(glob.glob(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")[0])\n",
    "# plt.imshow(img)#discard\n",
    "\n",
    "#how many?\n",
    "# tot_instances=0\n",
    "\n",
    "# studies=(os.listdir(PATH_TRAIN))\n",
    "# studies=sorted(studies)\n",
    "# print(\"tot_studies= \"+str(len(studies)))\n",
    "\n",
    "# tot_series=0\n",
    "# for study in studies:\n",
    "#     pth=os.path.join(PATH_TRAIN,study)\n",
    "# #     print(pth)\n",
    "#     series=os.listdir(pth)\n",
    "#     tot_series+=len(series)\n",
    "#     for serie in series:\n",
    "#         tot_instances+=len(os.listdir(os.path.join(pth,serie)))\n",
    "# print(\"tot_series= \"+str(tot_series))\n",
    "# print(\"tot_instances= \"+str(tot_instances))\n",
    "\n",
    "\n",
    "# class RsnaDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self,df,transforms):\n",
    "#         super().__init__()\n",
    "#         self.df = df\n",
    "#         self.transforms = transforms\n",
    "    \n",
    "#     def __getitem__(self,index):      \n",
    "#         image_path = self.df.image_paths[index]\n",
    "#         data = self.df[self.df['ImagePath']==image_path]\n",
    "#         labels = data[target_columns].values.reshape(-1)\n",
    "#         image = get_img(image_path)\n",
    "#         image = convert_to_rgb(image)\n",
    "        \n",
    "#         if self.transforms:\n",
    "#             image = self.transforms(image=image)['image']\n",
    "            \n",
    "#         image = torch.tensor(image,dtype=torch.float)        \n",
    "#         labels = torch.tensor(labels,dtype=torch.float)\n",
    "        \n",
    "#         return image,labels\n",
    "           \n",
    "#     def __len__(self):\n",
    "#         return self.image_paths.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:02:47.342921Z",
     "start_time": "2020-10-18T22:02:47.341392Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#see what above class does\n",
    "# t_df.head()\n",
    "# df_tmp=t_df.values\n",
    "# df_tmp.shape\n",
    "\n",
    "# df0 = df_tmp[df_tmp[:,3]==0]\n",
    "# df1 = df_tmp[df_tmp[:,3]==1]\n",
    "# print(len(df0))\n",
    "# print(len(df1))\n",
    "\n",
    "# df_tmp_balanced = np.concatenate([df0[:len(df1)],df1],axis=0)\n",
    "# print(len(df_tmp_balanced))\n",
    "# print(sum(df_tmp_balanced[:,3]==0))\n",
    "# print(sum(df_tmp_balanced[:,3]==1))\n",
    "# # df0 = self.df_main[self.df_main[:,3]==0]\n",
    "# #         df1 = self.df_main[self.df_main[:,3]==1]\n",
    "# #         np.random.shuffle(df0)\n",
    "# #         self.df = np.concatenate([df0[:len(df1)],df1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_training_augmentation(y=256,x=256):\n",
    "# #     train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "# #                            albu.VerticalFlip(p=0.5),\n",
    "# #                            albu.HorizontalFlip(p=0.5),\n",
    "# #                            albu.Downscale(p=1.0,scale_min=0.35,scale_max=0.75,),\n",
    "# #                            albu.Resize(y, x)]\n",
    "#     train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "#                            albu.HorizontalFlip(p=0.5),\n",
    "#                            albu.Resize(y, x)]\n",
    "#     return albu.Compose(train_transform)\n",
    "\n",
    "# def get_validation_augmentation(y=256,x=256):\n",
    "#     \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "#     test_transform = [albu.Resize(y, x)]\n",
    "#     return albu.Compose(test_transform)\n",
    "\n",
    "# formatted_settings = {\n",
    "#             'input_size': [3, 224, 224],\n",
    "#             'input_range': [0, 1],\n",
    "#             'mean': [0.485, 0.456, 0.406],\n",
    "#             'std': [0.229, 0.224, 0.225],}\n",
    "\n",
    "# def preprocess_to_train_format(dcm_data):\n",
    "#     '''\n",
    "#     preprocess the image (numpy array, into same format as the data used to train the model\n",
    "#     x: input image\n",
    "#     '''            #the following increases from 88 to 105 mins\n",
    "\n",
    "#     img = dcm_data.pixel_array * int(dcm_data.RescaleSlope) + int(dcm_data.RescaleIntercept)\n",
    "#     img = np.stack([window(img, WL=-600, WW=1500),\n",
    "#                       window(img, WL=40, WW=400),\n",
    "#                       window(img, WL=100, WW=700)], 2)\n",
    "\n",
    "#     img = image.astype(np.float32)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n",
    " \n",
    "#     #flips from BGR to RGB (or vice versa I'm not sure)\n",
    "#     if input_space == \"BGR\":\n",
    "#         x = x[..., ::-1].copy()\n",
    "#         gc.collect()\n",
    "\n",
    "#     if input_range is not None:\n",
    "#         if x.max() > 1 and input_range[1] == 1:\n",
    "#             x = x / 255.0\n",
    "\n",
    "#     return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T19:10:32.634142Z",
     "start_time": "2020-10-20T19:10:32.495800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "from pydicom.data import get_testdata_files\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "file_path = '../input/rsna-str-pulmonary-embolism-detection/test/00268ff88746/75d23269adbd/012c12fe09c3.dcm'\n",
    "dataset = pydicom.dcmread(file_path)\n",
    "\n",
    "\n",
    "if 'PixelData' in dataset:\n",
    "    rows = int(dataset.Rows)\n",
    "    cols = int(dataset.Columns)\n",
    "    print(\"Image size.......: {rows:d} x {cols:d}, {size:d} bytes\".format(\n",
    "        rows=rows, cols=cols, size=len(dataset.PixelData)))\n",
    "    if 'PixelSpacing' in dataset:\n",
    "        print(\"Pixel spacing....:\", dataset.PixelSpacing)\n",
    "\n",
    "# use .get() if not sure the item exists, and want a default value if missing\n",
    "print(\"Slice location...:\", dataset.get('SliceLocation', \"(missing)\"))\n",
    "\n",
    "# plot the image using matplotlib\n",
    "plt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('fastbook': conda)",
   "language": "python",
   "name": "python37764bitfastbookconda806f9a725aef497b9b1418421455f8a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "357.5px",
    "left": "637.014px",
    "right": "20px",
    "top": "480.938px",
    "width": "576.25px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
