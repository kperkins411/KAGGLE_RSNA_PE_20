{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Added a preconverted jpg data library to kernel from https://www.kaggle.com/vaillant/discussion see explanation for RGB channels<BR>\n",
    "Starter code from https://www.kaggle.com/orkatz2/pulmonary-embolism-pytorch-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:21:49.609786Z",
     "start_time": "2020-10-21T23:21:49.606753Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import cv2\n",
    "import albumentations as albu\n",
    "import functools\n",
    "import glob\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:19:17.839375Z",
     "start_time": "2020-10-21T23:19:17.835959Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:19:21.040832Z",
     "start_time": "2020-10-21T23:19:19.646177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the data\n",
    "train_df = pd.read_csv(\"../input/rsna-str-pulmonary-embolism-detection/train.csv\")\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:19:27.770575Z",
     "start_time": "2020-10-21T23:19:23.028689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 232.24 MB\n",
      "Memory usage after optimization is: 131.97 MB\n",
      "Decreased by 43.2%\n"
     ]
    }
   ],
   "source": [
    "from snippets import reduce_mem_usage\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:19:32.817130Z",
     "start_time": "2020-10-21T23:19:32.815664Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets get the precomputed images\n",
    "# PATH = '../input/rsna-str-pulmonary-embolism-detection'\n",
    "# PATH_TRAIN=PATH+'/train/'\n",
    "jpeg_dir = '../input/rsna-str-pe-detection-jpeg-256/train-jpegs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:02:09.448594Z",
     "start_time": "2020-10-20T20:02:09.410277Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_training_augmentation(y=256,x=256):\n",
    "# #     train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "# #                            albu.VerticalFlip(p=0.5),\n",
    "# #                            albu.HorizontalFlip(p=0.5),\n",
    "# #                            albu.Downscale(p=1.0,scale_min=0.35,scale_max=0.75,),\n",
    "# #                            albu.Resize(y, x)]\n",
    "#     train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "#                            albu.HorizontalFlip(p=0.5),\n",
    "#                            albu.Resize(y, x)]\n",
    "#     return albu.Compose(train_transform)\n",
    "\n",
    "# def get_validation_augmentation(y=256,x=256):\n",
    "#     \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "#     test_transform = [albu.Resize(y, x)]\n",
    "#     return albu.Compose(test_transform)\n",
    "\n",
    "# formatted_settings = {\n",
    "#             'input_size': [3, 224, 224],\n",
    "#             'input_range': [0, 1],\n",
    "#             'mean': [0.485, 0.456, 0.406],\n",
    "#             'std': [0.229, 0.224, 0.225],}\n",
    "\n",
    "# def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n",
    "#     if input_space == \"BGR\":\n",
    "#         x = x[..., ::-1].copy()\n",
    "#         gc.collect()\n",
    "\n",
    "#     if input_range is not None:\n",
    "#         if x.max() > 1 and input_range[1] == 1:\n",
    "#             x = x / 255.0\n",
    "\n",
    "#     if mean is not None:\n",
    "#         mean = np.array(mean)\n",
    "#         x = x - mean\n",
    "\n",
    "#     if std is not None:\n",
    "#         std = np.array(std)\n",
    "#         x = x / std\n",
    "\n",
    "#     return x\n",
    "\n",
    "# def get_preprocessing(preprocessing_fn):\n",
    "#     _transform = [\n",
    "#         albu.Lambda(image=preprocessing_fn),\n",
    "#         albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "#     ]\n",
    "#     return albu.Compose(_transform)\n",
    "\n",
    "\n",
    "\n",
    "# def to_tensor(x, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Convert image or mask.\n",
    "#     \"\"\"\n",
    "#     return x.transpose(2, 0, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:25:39.389785Z",
     "start_time": "2020-10-21T23:25:39.367452Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_train = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "preprocessing_val = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "class CTDatasetJPEG(Dataset):\n",
    "    def __init__(self,df,path,transforms=None,preprocessing=None,size=254,mode='val'):\n",
    "        \n",
    "        #get a numpy representation of the pandas dataframe\n",
    "        self.df_main = df.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "        self.size=size\n",
    "    \n",
    "        #either use all the validation data as given\n",
    "        #or generate a balanced set\n",
    "        if mode=='val':\n",
    "            self.df = self.df_main\n",
    "        else:\n",
    "            self.generate_balanced_set()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        returns the image and a label\n",
    "        '''\n",
    "        row = self.df[idx]\n",
    "        img = cv2.imread(glob.glob(f'{self.path}/{row[0]}/{row[1]}/*{row[2]}.jpg')[0])\n",
    "        \n",
    "        #lets flip from BGR to RGB\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "      \n",
    "        label = row[3:].astype(int)#pe_present_on_image \n",
    "        label[2:] = label[2:] if label[0]==1 else 0\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        if self.preprocessing:\n",
    "            img = self.preprocessing(img)\n",
    "            \n",
    "        return img,torch.from_numpy(label.reshape(-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    #this function gets a balanced set, 1/2 have pe_present_on_image=1, 1/2 have pe_present_on_image=0\n",
    "    #note that we discard a bunch of images that have no pe present\n",
    "    def generate_balanced_set(self):\n",
    "        df0 = self.df_main[self.df_main[:,3]==0]\n",
    "        df1 = self.df_main[self.df_main[:,3]==1]\n",
    "        np.random.shuffle(df0)\n",
    "        self.df = np.concatenate([df0[:len(df1)],df1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:25:54.710552Z",
     "start_time": "2020-10-21T23:25:54.521253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527158\n",
      "263436\n"
     ]
    }
   ],
   "source": [
    "#get a list of unque Studies\n",
    "StudyInstanceUID = list(set(train_df['StudyInstanceUID']))\n",
    "# print(len(StudyInstanceUID))\n",
    "\n",
    "#create train and val sets\n",
    "#TODO change back to train on full dataset, or to do an even mix of PE and nonPE images\n",
    "t_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[0:6200])]\n",
    "v_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[6200:])]\n",
    "print(len(t_df))\n",
    "print(len(v_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:26:13.133350Z",
     "start_time": "2020-10-21T23:26:09.797436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CTDatasetJPEG(t_df,jpeg_dir,\n",
    "                            transforms=preprocessing_train,mode='train')\n",
    "val_dataset = CTDatasetJPEG(v_df,jpeg_dir,\n",
    "                            transforms=preprocessing_val,mode='val')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:26:28.189714Z",
     "start_time": "2020-10-21T23:26:28.180344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img,label=val_dataset[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:26:43.228365Z",
     "start_time": "2020-10-21T23:26:43.225108Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         ...,\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         ...,\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T23:27:38.412596Z",
     "start_time": "2020-10-21T23:27:38.403875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>SeriesInstanceUID</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>pe_present_on_image</th>\n",
       "      <th>negative_exam_for_pe</th>\n",
       "      <th>qa_motion</th>\n",
       "      <th>qa_contrast</th>\n",
       "      <th>flow_artifact</th>\n",
       "      <th>rv_lv_ratio_gte_1</th>\n",
       "      <th>rv_lv_ratio_lt_1</th>\n",
       "      <th>leftsided_pe</th>\n",
       "      <th>chronic_pe</th>\n",
       "      <th>true_filling_defect_not_pe</th>\n",
       "      <th>rightsided_pe</th>\n",
       "      <th>acute_and_chronic_pe</th>\n",
       "      <th>central_pe</th>\n",
       "      <th>indeterminate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>c0f3cb036d06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>f57ffd3883b6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>41220fda34a3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>13b685b4b14f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6897fa9de148</td>\n",
       "      <td>2bfbb7fd2e8b</td>\n",
       "      <td>be0b7524ffb4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   c0f3cb036d06                    0   \n",
       "1     6897fa9de148      2bfbb7fd2e8b   f57ffd3883b6                    0   \n",
       "2     6897fa9de148      2bfbb7fd2e8b   41220fda34a3                    0   \n",
       "3     6897fa9de148      2bfbb7fd2e8b   13b685b4b14f                    0   \n",
       "4     6897fa9de148      2bfbb7fd2e8b   be0b7524ffb4                    0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                     0          0            0              0   \n",
       "1                     0          0            0              0   \n",
       "2                     0          0            0              0   \n",
       "3                     0          0            0              0   \n",
       "4                     0          0            0              0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  leftsided_pe  chronic_pe  \\\n",
       "0                  0                 1             1           0   \n",
       "1                  0                 1             1           0   \n",
       "2                  0                 1             1           0   \n",
       "3                  0                 1             1           0   \n",
       "4                  0                 1             1           0   \n",
       "\n",
       "   true_filling_defect_not_pe  rightsided_pe  acute_and_chronic_pe  \\\n",
       "0                           0              1                     0   \n",
       "1                           0              1                     0   \n",
       "2                           0              1                     0   \n",
       "3                           0              1                     0   \n",
       "4                           0              1                     0   \n",
       "\n",
       "   central_pe  indeterminate  \n",
       "0           0              0  \n",
       "1           0              0  \n",
       "2           0              0  \n",
       "3           0              0  \n",
       "4           0              0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:05.855330Z",
     "start_time": "2020-10-18T22:09:05.853444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165642\n",
      "269828\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T19:32:35.964116Z",
     "start_time": "2020-10-20T19:32:35.961972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    model_name=\"resnet18\"\n",
    "    batch_size = 256\n",
    "    WORKERS = 4\n",
    "    numb_classes =14\n",
    "    resume = False\n",
    "    epochs = 10\n",
    "    MODEL_PATH = 'log/cpt'\n",
    "    \n",
    "if not os.path.exists(config.MODEL_PATH):\n",
    "        os.makedirs(config.MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:20.946216Z",
     "start_time": "2020-10-18T22:09:20.944060Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.WORKERS, pin_memory=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=config.batch_size*2, shuffle=False, num_workers=config.WORKERS, pin_memory=True)\n",
    "\n",
    "# len(val_dl)\n",
    "# x,y = train_dataset[-400]\n",
    "# print(x.shape,len(y),y,len(train_dataset))\n",
    "\n",
    "# del x\n",
    "# del y\n",
    "\n",
    "# !free -m\n",
    "\n",
    "# len(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create, train and save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:28.674458Z",
     "start_time": "2020-10-18T22:09:28.454276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#where should it go\n",
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:37.150620Z",
     "start_time": "2020-10-18T22:09:36.208823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classes = len(target_columns)\n",
    "model = models.resnet18(pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features,config.numb_classes)\n",
    "model=model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:44.660156Z",
     "start_time": "2020-10-18T22:09:44.657923Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4,weight_decay= 0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max= 300,eta_min= 0.000001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:52.164825Z",
     "start_time": "2020-10-18T22:09:52.162839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:09:59.702760Z",
     "start_time": "2020-10-18T22:09:59.701161Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:10:07.241506Z",
     "start_time": "2020-10-18T22:10:07.231219Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ValMonitor():\n",
    "    def __init__(self, max_consecutive_increases):\n",
    "        '''\n",
    "        stops validation when loss has increased max_consecutive_increases\n",
    "        also indicates if its time to save the model\n",
    "        :param max_consecutive_increases: how many val increases before we should stop\n",
    "        '''\n",
    "        self.max_consecutive_increases=max_consecutive_increases\n",
    "        self.stop_counter=0\n",
    "        self.old_val = None\n",
    "        self.should_stop_training=False\n",
    "\n",
    "    def time_to_save(self, new_val):\n",
    "        '''\n",
    "        determines if its time to save model\n",
    "        :param new_val:\n",
    "        :return:\n",
    "        '''\n",
    "        should_save_model = False\n",
    "        if( self.old_val is None):\n",
    "            self.old_val = new_val+1\n",
    "\n",
    "        if(new_val<self.old_val):\n",
    "            self.old_val=new_val\n",
    "            if(self.stop_counter>0):\n",
    "                self.stop_counter-=1\n",
    "            should_save_model = True    #save if validation is better than last\n",
    "        else:\n",
    "            self.stop_counter+=1\n",
    "\n",
    "        #stop if validtion loss is rising\n",
    "        self.should_stop_training= (self.stop_counter>=self.max_consecutive_increases)\n",
    "        return should_save_model\n",
    "\n",
    "    def time_to_stop(self):\n",
    "        '''\n",
    "        determines if its time to stop training\n",
    "        :param new_val:\n",
    "        :return:\n",
    "        '''\n",
    "        return self.should_stop_training\n",
    "    \n",
    "\n",
    "class Trainer():\n",
    "    \n",
    "    def __init__(self,model, criterian,optimizer,scheduler,load_old_model=True, num_epochs=10):\n",
    "        self.model = model\n",
    "        self.load_old_model = load_old_model\n",
    "        self.criterian = criterian      \n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.num_epochs=num_epochs\n",
    "        self.vm= ValMonitor(2)\n",
    "        \n",
    "    def _train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        for batch_idx, (batch_imgs,batch_labels) in enumerate(tqdm_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_imgs, batch_labels = batch_imgs.to(dev).float(), batch_labels.to(dev).float()        \n",
    "            predicted = self.model(batch_imgs)\n",
    "            loss = self.criterian(predicted.float(), batch_labels)       \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            del batch_imgs\n",
    "            del batch_labels\n",
    " \n",
    "            current_loss_mean = (current_loss_mean * batch_idx + loss.item()) / (batch_idx + 1)\n",
    "            tqdm_loader.set_description('loss: {:.4} lr:{:.6}'.format(\n",
    "                    current_loss_mean, self.optimizer.param_groups[0]['lr']))\n",
    "            self.scheduler.step(batch_idx)\n",
    "            del loss\n",
    "        return current_loss_mean \n",
    "\n",
    "    def _val_epoch(self, loader):\n",
    "        self.model.eval()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader):\n",
    "            with torch.no_grad():\n",
    "                batch_imgs = imgs.to(dev).float()\n",
    "                batch_labels = labels.to(dev).float()\n",
    "                predicted = self.model(batch_imgs)\n",
    "                loss = self.criterian(predicted.float(),batch_labels.float()).item()\n",
    "                current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "                \n",
    "                del batch_imgs\n",
    "                del batch_labels\n",
    "        print(f'Validation loss {current_loss_mean}')\n",
    "        return current_loss_mean   \n",
    "    \n",
    "    \n",
    "#     def predict(self,imgs_tensor,get_fet = False):\n",
    "#         batch_imgs = batch_imgs.to(dev)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             res= torch.sigmoid(self.model(batch_imgs)) \n",
    "#         return predicted.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    def train(self,train_loader, val_loader):\n",
    "#         old_current_loss_mean_val = None#initialize to bus low number\n",
    "        best_val_loss = None\n",
    "        val_rise_tracker=1 #initialize\n",
    "        BAIL_WHEN_VAL_RISES_THIS_MANY_TIMES =2\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f'----- Epoch {epoch} -----')           \n",
    "            # Each epoch has a training and validation phase      \n",
    "            train_loader.dataset.generate_balanced_set()\n",
    "            \n",
    "            current_loss_mean_train = self._train_epoch(train_loader)          \n",
    "            current_loss_mean_val = self._val_epoch(val_loader)\n",
    "             \n",
    "            #initialize\n",
    "            if( self.vm.time_to_save(current_loss_mean_val)):\n",
    "                print(\"saving model\")\n",
    "                torch.save(self.model.state_dict(),config.MODEL_PATH+\"/{}_best.pth\".format(config.model_name))\n",
    "            \n",
    "            if(self.vm.time_to_stop()):\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T22:10:14.758667Z",
     "start_time": "2020-10-18T22:10:14.757153Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(model, criterion,optimizer,scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-18T23:05:20.159483Z",
     "start_time": "2020-10-18T22:10:22.272517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 0 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89da8a2224cc4a1cba529fab33c26a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09247a36394672abe42887d1d47ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.14332724610435296\n",
      "saving model\n",
      "----- Epoch 1 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca56b7b9b08449b1948c87690d1f78ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f39329efb264e4d9be28cf4fec07367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.1527683661746173\n",
      "----- Epoch 2 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af92e013ff40ea8f2fd8ad0746a593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6084ef5593b84acd925e4ba3fca0a152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.1259488269360824\n",
      "saving model\n",
      "----- Epoch 3 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738388a0f054468aaa22b1159b745fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a3a58897f64f2688037dfbb2651e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.12377547187806516\n",
      "saving model\n",
      "----- Epoch 4 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b893c0ec4d2c48f18e4dc30f7c05e9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f40a1896aa8480985b56ef2e6e10116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.12734036495914366\n",
      "----- Epoch 5 -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5400b3e6482747afa6b3f97dfd7af8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f14b28b954bf4bc55fd200e5147d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation loss 0.13471229909919194\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dl,val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction generator\n",
    "\n",
    "See KP_RSNA_gen_submission.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for img, lbl in train_dl:\n",
    "#     print(img[0].shape)\n",
    "#     print(len(lbl[0]))\n",
    "#     break\n",
    "# # tmp=next((train_dl))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# row = train_dataset.df[0]\n",
    "# print(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")\n",
    "# img = cv2.imread(glob.glob(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")[0])\n",
    "# plt.imshow(img)#discard\n",
    "\n",
    "#how many?\n",
    "# tot_instances=0\n",
    "\n",
    "# studies=(os.listdir(PATH_TRAIN))\n",
    "# studies=sorted(studies)\n",
    "# print(\"tot_studies= \"+str(len(studies)))\n",
    "\n",
    "# tot_series=0\n",
    "# for study in studies:\n",
    "#     pth=os.path.join(PATH_TRAIN,study)\n",
    "# #     print(pth)\n",
    "#     series=os.listdir(pth)\n",
    "#     tot_series+=len(series)\n",
    "#     for serie in series:\n",
    "#         tot_instances+=len(os.listdir(os.path.join(pth,serie)))\n",
    "# print(\"tot_series= \"+str(tot_series))\n",
    "# print(\"tot_instances= \"+str(tot_instances))\n",
    "\n",
    "\n",
    "# class RsnaDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self,df,transforms):\n",
    "#         super().__init__()\n",
    "#         self.df = df\n",
    "#         self.transforms = transforms\n",
    "    \n",
    "#     def __getitem__(self,index):      \n",
    "#         image_path = self.df.image_paths[index]\n",
    "#         data = self.df[self.df['ImagePath']==image_path]\n",
    "#         labels = data[target_columns].values.reshape(-1)\n",
    "#         image = get_img(image_path)\n",
    "#         image = convert_to_rgb(image)\n",
    "        \n",
    "#         if self.transforms:\n",
    "#             image = self.transforms(image=image)['image']\n",
    "            \n",
    "#         image = torch.tensor(image,dtype=torch.float)        \n",
    "#         labels = torch.tensor(labels,dtype=torch.float)\n",
    "        \n",
    "#         return image,labels\n",
    "           \n",
    "#     def __len__(self):\n",
    "#         return self.image_paths.shape[0]\n",
    "\n",
    "#see what above class does\n",
    "# t_df.head()\n",
    "# df_tmp=t_df.values\n",
    "# df_tmp.shape\n",
    "\n",
    "# df0 = df_tmp[df_tmp[:,3]==0]\n",
    "# df1 = df_tmp[df_tmp[:,3]==1]\n",
    "# print(len(df0))\n",
    "# print(len(df1))\n",
    "\n",
    "# df_tmp_balanced = np.concatenate([df0[:len(df1)],df1],axis=0)\n",
    "# print(len(df_tmp_balanced))\n",
    "# print(sum(df_tmp_balanced[:,3]==0))\n",
    "# print(sum(df_tmp_balanced[:,3]==1))\n",
    "# # df0 = self.df_main[self.df_main[:,3]==0]\n",
    "# #         df1 = self.df_main[self.df_main[:,3]==1]\n",
    "# #         np.random.shuffle(df0)\n",
    "# #         self.df = np.concatenate([df0[:len(df1)],df1],axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "fastbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "357.5px",
    "left": "-254.003px",
    "right": "20px",
    "top": "373.958px",
    "width": "576.285px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
