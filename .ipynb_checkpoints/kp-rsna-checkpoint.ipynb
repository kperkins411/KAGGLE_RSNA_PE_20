{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Added a preconverted jpg data library to kernel from https://www.kaggle.com/vaillant/discussion see explanation for RGB channels<BR>\n",
    "Starter code from https://www.kaggle.com/orkatz2/pulmonary-embolism-pytorch-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T14:29:50.332382Z",
     "start_time": "2020-10-16T14:29:50.173064Z"
    },
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T14:29:54.015575Z",
     "start_time": "2020-10-16T14:29:53.510268Z"
    },
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4389876b2bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malbu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import cv2\n",
    "import albumentations as albu\n",
    "import functools\n",
    "import glob\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "train_df = pd.read_csv(\"../input/rsna-str-pulmonary-embolism-detection/train.csv\")\n",
    "# test_df = pd.read_csv(\"../input/rsna-str-pulmonary-embolism-detection/test.csv\")\n",
    "# train.head()\n",
    "# len(train_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 232.24 MB\n",
      "Memory usage after optimization is: 131.97 MB\n",
      "Decreased by 43.2%\n"
     ]
    }
   ],
   "source": [
    "from snippets import reduce_mem_usage\n",
    "train_df = reduce_mem_usage(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../input/rsna-str-pulmonary-embolism-detection'\n",
    "PATH_TRAIN=PATH+'/train/'\n",
    "jpeg_dir = '../input/rsna-str-pe-detection-jpeg-256/train-jpegs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#where should it go\n",
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation(y=256,x=256):\n",
    "    train_transform = [albu.RandomBrightnessContrast(p=0.3),\n",
    "                           albu.VerticalFlip(p=0.5),\n",
    "                           albu.HorizontalFlip(p=0.5),\n",
    "                           albu.Downscale(p=1.0,scale_min=0.35,scale_max=0.75,),\n",
    "                           albu.Resize(y, x)]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "formatted_settings = {\n",
    "            'input_size': [3, 224, 224],\n",
    "            'input_range': [0, 1],\n",
    "            'mean': [0.485, 0.456, 0.406],\n",
    "            'std': [0.229, 0.224, 0.225],}\n",
    "\n",
    "def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "        gc.collect()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 255.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)\n",
    "\n",
    "def get_validation_augmentation(y=256,x=256):\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [albu.Resize(y, x)]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Convert image or mask.\n",
    "    \"\"\"\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "class CTDataset2D(Dataset):\n",
    "    def __init__(self,df,transforms = albu.Compose([albu.HorizontalFlip()]),preprocessing=None,size=256,mode='val'):\n",
    "        \n",
    "        #get a numpy representation of the pandas dataframe\n",
    "        self.df_main = df.values\n",
    "        \n",
    "        #either use all the validation data as given\n",
    "        #or generate a balanced set\n",
    "        if mode=='val':\n",
    "            self.df = self.df_main\n",
    "        else:\n",
    "            self.generate_balanced_set()\n",
    "            \n",
    "        self.transforms = transforms\n",
    "        self.preprocessing = preprocessing\n",
    "        self.size=size\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        img = cv2.imread(glob.glob(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")[0])\n",
    "        label = row[3:].astype(int)\n",
    "        label[2:] = label[2:] if label[0]==1 else 0\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        if self.preprocessing:\n",
    "            img = self.preprocessing(image=img)['image']\n",
    "        return img,torch.from_numpy(label.reshape(-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    #this function gets a balanced set, 1/2 have pe_present_on_image=1, 1/2 have pe_present_on_image=0\n",
    "    #note that we discard a bunch of images that have no pe present\n",
    "    def generate_balanced_set(self):\n",
    "        df0 = self.df_main[self.df_main[:,3]==0]\n",
    "        df1 = self.df_main[self.df_main[:,3]==1]\n",
    "        np.random.shuffle(df0)\n",
    "        self.df = np.concatenate([df0[:len(df1)],df1],axis=0)\n",
    "        \n",
    "\n",
    "def norm(img):\n",
    "    img-=img.min()\n",
    "    return img/img.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what above class does\n",
    "# t_df.head()\n",
    "# df_tmp=t_df.values\n",
    "# df_tmp.shape\n",
    "\n",
    "# df0 = df_tmp[df_tmp[:,3]==0]\n",
    "# df1 = df_tmp[df_tmp[:,3]==1]\n",
    "# print(len(df0))\n",
    "# print(len(df1))\n",
    "\n",
    "# df_tmp_balanced = np.concatenate([df0[:len(df1)],df1],axis=0)\n",
    "# print(len(df_tmp_balanced))\n",
    "# print(sum(df_tmp_balanced[:,3]==0))\n",
    "# print(sum(df_tmp_balanced[:,3]==1))\n",
    "# # df0 = self.df_main[self.df_main[:,3]==0]\n",
    "# #         df1 = self.df_main[self.df_main[:,3]==1]\n",
    "# #         np.random.shuffle(df0)\n",
    "# #         self.df = np.concatenate([df0[:len(df1)],df1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1473582"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StudyInstanceUID = list(set(train_df['StudyInstanceUID']))\n",
    "# print(len(StudyInstanceUID))\n",
    "\n",
    "#create train and val sets\n",
    "#TODO change back to train on full dataset, or to do an even mix of PE and nonPE images\n",
    "t_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[0:6000])]\n",
    "v_df = train_df[train_df['StudyInstanceUID'].isin(StudyInstanceUID[6000:])]\n",
    "len(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    model_name=\"resnet18\"\n",
    "    batch_size = 64\n",
    "    WORKERS = 4\n",
    "    classes =14\n",
    "    resume = False\n",
    "    epochs = 10\n",
    "    MODEL_PATH = 'log/cpt'\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.7/site-packages (0.57.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from memory_profiler) (5.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preprocessing_fn = functools.partial(preprocess_input, **formatted_settings)\n",
    "train_dataset = CTDataset2D(t_df,\n",
    "                            transforms=get_training_augmentation(),\n",
    "                            preprocessing=get_preprocessing(preprocessing_fn),mode='train')\n",
    "val_dataset = CTDataset2D(v_df,\n",
    "                            transforms=get_validation_augmentation(),\n",
    "                            preprocessing=get_preprocessing(preprocessing_fn))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del t_df\n",
    "del v_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=config.WORKERS, pin_memory=True)\n",
    "val = DataLoader(val_dataset, batch_size=config.batch_size*2, shuffle=False, num_workers=config.WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 256, 256), 14, tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]), 159718)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = train_dataset[-400]\n",
    "x.shape,len(y),y,len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    model_name=\"resnet18\"\n",
    "    batch_size = 128\n",
    "    WORKERS = 4\n",
    "    numb_classes =14\n",
    "    resume = False\n",
    "    epochs = 10\n",
    "    MODEL_PATH = 'log/cpt'\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b41c2935d4d4c349adc629dff3edfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# classes = len(target_columns)\n",
    "model = models.resnet18(pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features,config.numb_classes)\n",
    "model=model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4,weight_decay= 0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max= 300,eta_min= 0.000001)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(self,loss_fn,model,optimizer,scheduler):\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "        \n",
    "    def batch_train(self, batch_imgs, batch_labels, batch_idx):\n",
    "        #self.model.train() done in train_epoch()\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_imgs, batch_labels = batch_imgs.to(dev).float(), batch_labels.to(dev).float()       \n",
    "        predicted = self.model(batch_imgs)\n",
    "        loss = self.loss_fn(predicted.float(), batch_labels)\n",
    "        \n",
    "        del batch_imgs\n",
    "        del batch_labels\n",
    "        gc.collect()\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item(), predicted\n",
    "    \n",
    "    def batch_valid(self, batch_imgs,get_fet):\n",
    "        # KP modify\n",
    "        #self.model.eval() done in valid_epoch\n",
    "        batch_imgs = batch_imgs.to(dev)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            res= torch.sigmoid(self.model(batch_imgs))\n",
    "        \n",
    "        del batch_imgs\n",
    "        gc.collect()\n",
    "        return res\n",
    "    \n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader):\n",
    "            loss, predicted = self.batch_train(imgs, labels, batch_idx)\n",
    "            current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "            tqdm_loader.set_description('loss: {:.4} lr:{:.6}'.format(\n",
    "                    current_loss_mean, self.optimizer.param_groups[0]['lr']))\n",
    "            self.scheduler.step(batch_idx)\n",
    "        return current_loss_mean\n",
    "    \n",
    "    def valid_epoch(self, loader,name=\"valid\"):\n",
    "        self.model.eval()\n",
    "        tqdm_loader = tqdm(loader)\n",
    "        current_loss_mean = 0\n",
    "        for batch_idx, (imgs,labels) in enumerate(tqdm_loader):\n",
    "            with torch.no_grad():\n",
    "                batch_imgs = imgs.to(dev).float()\n",
    "                batch_labels = labels.to(dev)\n",
    "                predicted = self.model(batch_imgs)\n",
    "                loss = self.loss_fn(predicted.float(),batch_labels.float()).item()\n",
    "                current_loss_mean = (current_loss_mean * batch_idx + loss) / (batch_idx + 1)\n",
    "        score = 1-current_loss_mean\n",
    "        print('metric {}'.format(score))\n",
    "        return score\n",
    "    \n",
    "    def run(self,train_loder,val_loder):\n",
    "        best_score = -100000\n",
    "        for e in range(config.epochs):\n",
    "            print(\"----------Epoch {}-----------\".format(e))\n",
    "            current_loss_mean = self.train_epoch(train_loder)\n",
    "            train_loder.dataset.generate_balanced_set()\n",
    "            score = self.valid_epoch(val_loder)\n",
    "            if best_score < score:\n",
    "                best_score = score\n",
    "                torch.save(self.model.state_dict(),config.MODEL_PATH+\"/{}_best.pth\".format(config.model_name))\n",
    "                \n",
    "    def batch_valid_tta(self, batch_imgs):\n",
    "        batch_imgs = batch_imgs.to(dev)\n",
    "        gc.collect()\n",
    "        predicted = model(batch_imgs)\n",
    "        tta_flip = [[-1],[-2]]\n",
    "        for axis in tta_flip:\n",
    "            predicted += torch.flip(model(torch.flip(batch_imgs, axis)), axis)\n",
    "        predicted = predicted/(1+len(tta_flip))\n",
    "        predicted = torch.sigmoid(predicted)\n",
    "        return predicted.cpu().numpy()\n",
    "    \n",
    "    def load_best_model(self):\n",
    "        if os.path.exists(config.MODEL_PATH+\"/{}_best.pth\".format(config.model_name)):\n",
    "            self.model.load_state_dict(torch.load(config.MODEL_PATH+\"/{}_best.pth\".format(config.model_name)))\n",
    "    \n",
    "    def predict(self,imgs_tensor,get_fet = False):\n",
    "        self.model.train()\n",
    "        with torch.no_grad():\n",
    "            return self.batch_valid(imgs_tensor,get_fet=get_fet)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer = trainer(loss_fn,model,optimizer,scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 0-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfeb7f4df394fec891694f433b5f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%mprun -f Trainer.run(train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Epoch 0-----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0681a746d6f34172a266145a12876da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2477.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a67c5c40612c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-b9694ff983f8>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_loder, val_loder)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----------Epoch {}-----------\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mcurrent_loss_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mtrain_loder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_balanced_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b9694ff983f8>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mcurrent_loss_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mcurrent_loss_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss_mean\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             tqdm_loader.set_description('loss: {:.4} lr:{:.6}'.format(\n",
      "\u001b[0;32m<ipython-input-18-b9694ff983f8>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_fet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Trainer.run(train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()\n",
    "import sys\n",
    "sys.getsizeof(model)\n",
    "# model\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# row = train_dataset.df[0]\n",
    "# print(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")\n",
    "# img = cv2.imread(glob.glob(f\"{jpeg_dir}/{row[0]}/{row[1]}/*{row[2]}.jpg\")[0])\n",
    "# plt.imshow(img)#discard\n",
    "\n",
    "#how many?\n",
    "# tot_instances=0\n",
    "\n",
    "# studies=(os.listdir(PATH_TRAIN))\n",
    "# studies=sorted(studies)\n",
    "# print(\"tot_studies= \"+str(len(studies)))\n",
    "\n",
    "# tot_series=0\n",
    "# for study in studies:\n",
    "#     pth=os.path.join(PATH_TRAIN,study)\n",
    "# #     print(pth)\n",
    "#     series=os.listdir(pth)\n",
    "#     tot_series+=len(series)\n",
    "#     for serie in series:\n",
    "#         tot_instances+=len(os.listdir(os.path.join(pth,serie)))\n",
    "# print(\"tot_series= \"+str(tot_series))\n",
    "# print(\"tot_instances= \"+str(tot_instances))\n",
    "\n",
    "\n",
    "# class RsnaDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self,df,transforms):\n",
    "#         super().__init__()\n",
    "#         self.df = df\n",
    "#         self.transforms = transforms\n",
    "    \n",
    "#     def __getitem__(self,index):      \n",
    "#         image_path = self.df.image_paths[index]\n",
    "#         data = self.df[self.df['ImagePath']==image_path]\n",
    "#         labels = data[target_columns].values.reshape(-1)\n",
    "#         image = get_img(image_path)\n",
    "#         image = convert_to_rgb(image)\n",
    "        \n",
    "#         if self.transforms:\n",
    "#             image = self.transforms(image=image)['image']\n",
    "            \n",
    "#         image = torch.tensor(image,dtype=torch.float)        \n",
    "#         labels = torch.tensor(labels,dtype=torch.float)\n",
    "        \n",
    "#         return image,labels\n",
    "           \n",
    "#     def __len__(self):\n",
    "#         return self.image_paths.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
